{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e675a4",
   "metadata": {},
   "source": [
    "Okay, so we've collected some data. Woohoo. The first thing we'll want to do is take the files our acquisition software has just output and organize them in a sensible way. \n",
    "\n",
    "What is \"a sensible way,\" you mught ask? Any organization where you (and anybody else who ever needs to use your data) will immediately know where everything is and be able to find all the important metadata (e.g. acquisition parameters, which may ont be available in the file itself, etc.).\n",
    "\n",
    "Labs will often have their own internal guidelines for how an EEG dataset should be organized, or maybe people just do what works for them. I'm a big fan of the [Brain Imaging Data Structure (BIDS)](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/03-electroencephalography.html); if a committee of experienced researchers put their heads together and decided this was a sensible way to organize data, who am I to reinvent the wheel? And by organizing our data the same way as everyone else, we are afforded the ability to use tools that assume your data follows BIDS specifications. Importantly, when we write code to analyze our dataset, we know we can apply it to any other dataset stored in BIDS format in the future, like every dataset on [OpenNeuro](https://openneuro.org/) which can save you a ton of time down the line.\n",
    "\n",
    "The only annoying part about BIDS is that it can be a struggle to get your data into the highly specific directory structure. Luckily, the EEG ecosystem has tools like [MNE-BIDS](https://mne.tools/mne-bids/stable/index.html) in Python and [Fieldtrip Toolbox's `data2bids` function](https://www.fieldtriptoolbox.org/example/bids/) to make this part trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec100e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_bids import BIDSPath, write_raw_bids, get_anonymization_daysback\n",
    "import itertools\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb5460",
   "metadata": {},
   "source": [
    "First we'll load our data into MNE as we would any other EEG file. I've put the data we collected from Pablo into a folder called `data`, which we'll peak inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94ccd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letty_subj_6.eeg',\n",
       " 'letty_subj_5.eeg',\n",
       " 'letty_subj_4.eeg',\n",
       " 'letty_subj_3.eeg',\n",
       " 'letty_subj_2.eeg',\n",
       " 'letty_subj_3_2.vmrk',\n",
       " 'letty_subj_3_2.vhdr',\n",
       " 'letty_subj_6.vhdr',\n",
       " 'letty_subj_6.vmrk',\n",
       " 'CACS-64_NO_REF.bvef',\n",
       " 'letty_subj_4_2.vmrk',\n",
       " 'letty_subj_4_2.vhdr',\n",
       " 'letty_subj_4.vmrk',\n",
       " 'letty_subj_4.vhdr',\n",
       " 'letty_subj_4_2.eeg',\n",
       " 'letty_subj_5.vhdr',\n",
       " 'letty_subj_5.vmrk',\n",
       " 'letty_subj_2.vmrk',\n",
       " 'letty_subj_2.vhdr',\n",
       " 'letty_subj_3_2.eeg',\n",
       " 'letty_subj_3.vhdr',\n",
       " 'letty_subj_3.vmrk',\n",
       " 'CACS-64_REF.bvef']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw' # where our data currently lives\n",
    "BIDS_DIR = '../data/bids' # where we want it to live\n",
    "\n",
    "fnames = os.listdir(DATA_DIR)\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fe92c",
   "metadata": {},
   "source": [
    "We only have one subject now, so it would be easy to hardcode this. But we can save ourselves some work down the line by automating this process, so we'll pretend we have more subjects than we do. \n",
    "\n",
    "MNE only needs one of the file names to read the file; namely, the `.vhdr` header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331fcb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letty_subj_3_2.vhdr',\n",
       " 'letty_subj_6.vhdr',\n",
       " 'letty_subj_4_2.vhdr',\n",
       " 'letty_subj_4.vhdr',\n",
       " 'letty_subj_5.vhdr',\n",
       " 'letty_subj_2.vhdr',\n",
       " 'letty_subj_3.vhdr']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = [f for f in fnames if '.vhdr' in f] # filter for .vhdr files\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd29443",
   "metadata": {},
   "source": [
    "How our subject IDs and task names are represented in our filename will obviously vary from project to project, since they depend on what you type into the acquistion software. (This type of inconsistent naming conventions is why we're converting to BIDS to begin with.) So you'll need to write your own code for this next part. \n",
    "\n",
    "I'm using regular expressions because I normally find them handy for pulling info out of file names, but obviously there are other (easier, if you don't already know the notoriously inscrutable regular expression syntax) ways to do this. Don't mind me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "768fe601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '6', '4', '4', '5', '2', '3']\n",
      "['pitch', 'pitch', 'pitch', 'pitch', 'pitch', 'pitch', 'pitch']\n",
      "['2', '1', '2', '1', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "# Get subject list from file order\n",
    "filter_subs = re.compile('letty_subj_(\\w?).*') # create regex filter\n",
    "subs = list(map(filter_subs.findall, fnames)) # extract subject numbers with filter\n",
    "subs = list(itertools.chain(*subs)) # flatten then nested list\n",
    "print(subs)\n",
    "\n",
    "# Get a task list\n",
    "tasks = ['pitch']*len(subs) # broadcast the only task name\n",
    "print(tasks)\n",
    "\n",
    "# Get a run list\n",
    "filter_runs = re.compile('\\w+[0-9]_([0-9]).*')\n",
    "runs = list(map(filter_runs.findall, fnames))\n",
    "runs = ['1' if x == [] else x for x in runs]\n",
    "runs = list(itertools.chain(*runs))\n",
    "print(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92faeae9",
   "metadata": {},
   "source": [
    "We'll want to rename our channels to something more information than `'Ch1'`, etc. Brain Products' acticaps positions electrodes according to the [10-20 system](https://en.wikipedia.org/wiki/10%E2%80%9320_system_(EEG)), so if we rename our electrodes to their 10-20 location names, everyone will know where they are on the head. We'll make a mapping from the channel names in our files to the corresponding 10-20 names using the layout file provided by Brain Products for our cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390f0b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ch0': 'GND',\n",
       " 'Ch1': 'Fp1',\n",
       " 'Ch2': 'Fz',\n",
       " 'Ch3': 'F3',\n",
       " 'Ch4': 'F7',\n",
       " 'Ch5': 'FT9',\n",
       " 'Ch6': 'FC5',\n",
       " 'Ch7': 'FC1',\n",
       " 'Ch8': 'C3',\n",
       " 'Ch9': 'T7',\n",
       " 'Ch10': 'TP9',\n",
       " 'Ch11': 'CP5',\n",
       " 'Ch12': 'CP1',\n",
       " 'Ch13': 'Pz',\n",
       " 'Ch14': 'P3',\n",
       " 'Ch15': 'P7',\n",
       " 'Ch16': 'O1',\n",
       " 'Ch17': 'Oz',\n",
       " 'Ch18': 'O2',\n",
       " 'Ch19': 'P4',\n",
       " 'Ch20': 'P8',\n",
       " 'Ch21': 'TP10',\n",
       " 'Ch22': 'CP6',\n",
       " 'Ch23': 'CP2',\n",
       " 'Ch24': 'Cz',\n",
       " 'Ch25': 'C4',\n",
       " 'Ch26': 'T8',\n",
       " 'Ch27': 'FT10',\n",
       " 'Ch28': 'FC6',\n",
       " 'Ch29': 'FC2',\n",
       " 'Ch30': 'F4',\n",
       " 'Ch31': 'F8',\n",
       " 'Ch32': 'Fp2',\n",
       " 'Ch33': 'AF7',\n",
       " 'Ch34': 'AF3',\n",
       " 'Ch35': 'AFz',\n",
       " 'Ch36': 'F1',\n",
       " 'Ch37': 'F5',\n",
       " 'Ch38': 'FT7',\n",
       " 'Ch39': 'FC3',\n",
       " 'Ch40': 'C1',\n",
       " 'Ch41': 'C5',\n",
       " 'Ch42': 'TP7',\n",
       " 'Ch43': 'CP3',\n",
       " 'Ch44': 'P1',\n",
       " 'Ch45': 'P5',\n",
       " 'Ch46': 'PO7',\n",
       " 'Ch47': 'PO3',\n",
       " 'Ch48': 'POz',\n",
       " 'Ch49': 'PO4',\n",
       " 'Ch50': 'PO8',\n",
       " 'Ch51': 'P6',\n",
       " 'Ch52': 'P2',\n",
       " 'Ch53': 'CPz',\n",
       " 'Ch54': 'CP4',\n",
       " 'Ch55': 'TP8',\n",
       " 'Ch56': 'C6',\n",
       " 'Ch57': 'C2',\n",
       " 'Ch58': 'FC4',\n",
       " 'Ch59': 'FT8',\n",
       " 'Ch60': 'F6',\n",
       " 'Ch61': 'AF8',\n",
       " 'Ch62': 'AF4',\n",
       " 'Ch63': 'F2',\n",
       " 'Ch64': 'FCz'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig = mne.channels.read_custom_montage(DATA_DIR + '/CACS-64_NO_REF.bvef')\n",
    "mapping = {'Ch%s'%i: dig.ch_names[i] for i in range(len(dig.ch_names))}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0961ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve channel mapping\n",
    "# dig = mne.channels.read_custom_montage(DATA_DIR + '/CACS-64_NO_REF.bvef')\n",
    "# no_ref_mapping = {'Ch%s'%i: dig.ch_names[i] for i in range(len(dig.ch_names))}\n",
    "# no_ref_mapping\n",
    "\n",
    "# # Mapping for subjects 3 and 4\n",
    "# dig = mne.channels.read_custom_montage(DATA_DIR + '/CACS-64_REF.bvef')\n",
    "# ref_mapping = {'Ch%s'%i: dig.ch_names[i] for i in range(len(dig.ch_names))}\n",
    "# ref_mapping\n",
    "\n",
    "# # channel_mapping = accidental_mapping if sid in bad_mapping_subs else default_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ebafd",
   "metadata": {},
   "source": [
    "The ground electrode doesn't appear in the file, so we will remove that from the mapping (because MNE isn't yet smart enough to deal with extraneous values). Also, I had electrode 24 set as the reference electrode during the recording, so it didn't appear in the file. (As a side note, while we don't do it here, we can actually add the reference channel back with a constant value of zero, since it will become a valid channel again after re-referencing.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8638f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mapping['Ch0']\n",
    "del mapping['Ch24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82bf752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird_sub_channels = {\n",
    "#     bad_sub1: bs1_mapping_dict,\n",
    "#     bad_sub2: bs2_mapping_dict\n",
    "# }\n",
    "\n",
    "# channel_map = weird_sub_channels.get(sid, default_map)\n",
    "# for i, fname in enumerate(fnames):\n",
    "#     print(i, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0afa6",
   "metadata": {},
   "source": [
    "Now all we need to do is read the data file using MNE, add any info that BIDS will want but isn't available in the original file (in this case, the power line frequency, which varies from country to country so constitutes important and non-obvious metadata), give the basic info we just extracted to MNE-BIDS so it can build the BIDS directory structure, and copy our data. We'll also want to rename our events to something more interpretable than integer codes.\n",
    "\n",
    "If we want to share this dataset in the future, we'll also need to anonymize it. That means removing the date it is collected. (It would also mean removing our subject's name, but the cat's already out of the bag on that one in this case -- sorry, Pablo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d323f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from ../data/raw/letty_subj_3_2.vhdr...\n",
      "Setting channel info structure...\n",
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S  1', 'Stimulus/S  2', 'Stimulus/S  3', 'Stimulus/S  4', 'Stimulus/S  5']\n",
      "Extracting parameters from /Users/nusbaumlab/src/pitch_tracking/data/raw/letty_subj_3_2.vhdr...\n",
      "Setting channel info structure...\n",
      "Writing '../data/bids/participants.tsv'...\n",
      "Writing '../data/bids/participants.json'...\n",
      "Used Annotations descriptions: ['baseline', 'oddball']\n",
      "Writing '../data/bids/sub-3/eeg/sub-3_task-pitch_events.tsv'...\n",
      "Writing '../data/bids/dataset_description.json'...\n",
      "Writing '../data/bids/sub-3/eeg/sub-3_task-pitch_eeg.json'...\n",
      "Writing '../data/bids/sub-3/eeg/sub-3_task-pitch_channels.tsv'...\n",
      "Copying data files to sub-3_task-pitch_eeg.vhdr\n",
      "Created \"sub-3_task-pitch_eeg.eeg\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-3/eeg\".\n",
      "Created \"sub-3_task-pitch_eeg.vhdr\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-3/eeg\".\n",
      "Created \"sub-3_task-pitch_eeg.vmrk\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-3/eeg\".\n",
      "Anonymized all dates in VHDR and VMRK.\n",
      "Writing '../data/bids/sub-3/sub-3_scans.tsv'...\n",
      "Wrote ../data/bids/sub-3/sub-3_scans.tsv entry with eeg/sub-3_task-pitch_eeg.vhdr.\n",
      "Extracting parameters from ../data/raw/letty_subj_6.vhdr...\n",
      "Setting channel info structure...\n",
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S  1', 'Stimulus/S  2', 'Stimulus/S  3', 'Stimulus/S  4', 'Stimulus/S  5']\n",
      "Extracting parameters from /Users/nusbaumlab/src/pitch_tracking/data/raw/letty_subj_6.vhdr...\n",
      "Setting channel info structure...\n",
      "Writing '../data/bids/participants.tsv'...\n",
      "Writing '../data/bids/participants.json'...\n",
      "Used Annotations descriptions: ['baseline', 'oddball']\n",
      "Writing '../data/bids/sub-6/eeg/sub-6_task-pitch_events.tsv'...\n",
      "Writing '../data/bids/dataset_description.json'...\n",
      "Writing '../data/bids/sub-6/eeg/sub-6_task-pitch_eeg.json'...\n",
      "Writing '../data/bids/sub-6/eeg/sub-6_task-pitch_channels.tsv'...\n",
      "Copying data files to sub-6_task-pitch_eeg.vhdr\n",
      "Created \"sub-6_task-pitch_eeg.eeg\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-6/eeg\".\n",
      "Created \"sub-6_task-pitch_eeg.vhdr\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-6/eeg\".\n",
      "Created \"sub-6_task-pitch_eeg.vmrk\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-6/eeg\".\n",
      "Anonymized all dates in VHDR and VMRK.\n",
      "Writing '../data/bids/sub-6/sub-6_scans.tsv'...\n",
      "Wrote ../data/bids/sub-6/sub-6_scans.tsv entry with eeg/sub-6_task-pitch_eeg.vhdr.\n",
      "Extracting parameters from ../data/raw/letty_subj_4_2.vhdr...\n",
      "Setting channel info structure...\n",
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S  1', 'Stimulus/S  2', 'Stimulus/S  3', 'Stimulus/S  4', 'Stimulus/S  5']\n",
      "Extracting parameters from /Users/nusbaumlab/src/pitch_tracking/data/raw/letty_subj_4_2.vhdr...\n",
      "Setting channel info structure...\n",
      "Writing '../data/bids/participants.tsv'...\n",
      "Writing '../data/bids/participants.json'...\n",
      "Used Annotations descriptions: ['baseline', 'oddball']\n",
      "Writing '../data/bids/sub-4/eeg/sub-4_task-pitch_events.tsv'...\n",
      "Writing '../data/bids/dataset_description.json'...\n",
      "Writing '../data/bids/sub-4/eeg/sub-4_task-pitch_eeg.json'...\n",
      "Writing '../data/bids/sub-4/eeg/sub-4_task-pitch_channels.tsv'...\n",
      "Copying data files to sub-4_task-pitch_eeg.vhdr\n",
      "Created \"sub-4_task-pitch_eeg.eeg\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-4/eeg\".\n",
      "Created \"sub-4_task-pitch_eeg.vhdr\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-4/eeg\".\n",
      "Created \"sub-4_task-pitch_eeg.vmrk\" in \"/Users/nusbaumlab/src/pitch_tracking/data/bids/sub-4/eeg\".\n",
      "Anonymized all dates in VHDR and VMRK.\n",
      "Writing '../data/bids/sub-4/sub-4_scans.tsv'...\n",
      "Wrote ../data/bids/sub-4/sub-4_scans.tsv entry with eeg/sub-4_task-pitch_eeg.vhdr.\n",
      "Extracting parameters from ../data/raw/letty_subj_4.vhdr...\n",
      "Setting channel info structure...\n",
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S  1', 'Stimulus/S  2', 'Stimulus/S  3', 'Stimulus/S  4', 'Stimulus/S  5']\n",
      "Extracting parameters from /Users/nusbaumlab/src/pitch_tracking/data/raw/letty_subj_4.vhdr...\n",
      "Setting channel info structure...\n",
      "Writing '../data/bids/participants.tsv'...\n",
      "Writing '../data/bids/participants.json'...\n",
      "Used Annotations descriptions: ['baseline', 'oddball']\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "\"../data/bids/sub-4/eeg/sub-4_task-pitch_events.tsv\" already exists. Please set overwrite to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     daysback_min, daysback_max \u001b[38;5;241m=\u001b[39m get_anonymization_daysback(raw)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# write data into BIDS directory, while anonymizing\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mwrite_raw_bids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbids_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbids_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;43;03m#         allow_preload = True, # whether to load full dataset into memory when copying\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;43;03m#         format = 'BrainVision', # format to save to\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43manonymize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdaysback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdaysback_min\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# shift dates by daysback\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-592>:24\u001b[0m, in \u001b[0;36mwrite_raw_bids\u001b[0;34m(raw, bids_path, events_data, event_id, anonymize, format, symlink, empty_room, allow_preload, montage, acpc_aligned, overwrite, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pitch-tracking/lib/python3.10/site-packages/mne_bids/write.py:1561\u001b[0m, in \u001b[0;36mwrite_raw_bids\u001b[0;34m(raw, bids_path, events_data, event_id, anonymize, format, symlink, empty_room, allow_preload, montage, acpc_aligned, overwrite, verbose)\u001b[0m\n\u001b[1;32m   1558\u001b[0m events_array, event_dur, event_desc_id_map \u001b[38;5;241m=\u001b[39m _read_events(\n\u001b[1;32m   1559\u001b[0m     events_data, event_id, raw, bids_path\u001b[38;5;241m=\u001b[39mbids_path)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events_array\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1561\u001b[0m     \u001b[43m_events_tsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevents_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdurations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_dur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevents_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_desc_id_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m                \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;66;03m# Kepp events_array around for BrainVision writing below.\u001b[39;00m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m event_desc_id_map, events_data, event_id, event_dur\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pitch-tracking/lib/python3.10/site-packages/mne_bids/write.py:244\u001b[0m, in \u001b[0;36m_events_tsv\u001b[0;34m(events, durations, raw, fname, trial_type, overwrite)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 244\u001b[0m \u001b[43m_write_tsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-578>:24\u001b[0m, in \u001b[0;36m_write_tsv\u001b[0;34m(fname, dictionary, overwrite, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pitch-tracking/lib/python3.10/site-packages/mne_bids/utils.py:200\u001b[0m, in \u001b[0;36m_write_tsv\u001b[0;34m(fname, dictionary, overwrite, verbose)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"Write an ordered dictionary to a .tsv file.\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op\u001b[38;5;241m.\u001b[39mexists(fname) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m already exists. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    201\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease set overwrite to True.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    202\u001b[0m _to_tsv(dictionary, fname)\n\u001b[1;32m    204\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileExistsError\u001b[0m: \"../data/bids/sub-4/eeg/sub-4_task-pitch_events.tsv\" already exists. Please set overwrite to True."
     ]
    }
   ],
   "source": [
    "for i in range(len(fnames)):\n",
    "    \n",
    "    sub = subs[i]\n",
    "    task = tasks[i]\n",
    "    block = \n",
    "    fpath = os.path.join(DATA_DIR, fnames[i])\n",
    "    \n",
    "    # load data with MNE function for your file format\n",
    "    raw = mne.io.read_raw_brainvision(fpath)\n",
    "\n",
    "    # add some info BIDS will want\n",
    "    raw.info['line_freq'] = 60 # the power line frequency in the building we collected in\n",
    "    \n",
    "    # rename events from random integers to interpretable names\n",
    "    # (this part is specific to your experiment, obviously)\n",
    "    events, event_ids = mne.events_from_annotations(raw)\n",
    "    events = events[events[:,2] != event_ids['New Segment/'], :]\n",
    "    event_codes = events[:,2]\n",
    "    baseline_code = np.argmax(np.bincount(event_codes)) # the one with more trials\n",
    "    oddball_code = np.unique(event_codes)[np.unique(event_codes) != baseline_code][0]\n",
    "    event_names = {baseline_code: 'baseline', oddball_code: 'oddball'}\n",
    "    annot = mne.annotations_from_events(events, sfreq = raw.info['sfreq'], event_desc = event_names)\n",
    "    raw = raw.set_annotations(annot)\n",
    "#     raw.load_data() # read data from memory\n",
    "    raw.rename_channels(mapping)\n",
    "    \n",
    "    # build appropriate BIDS directory structure \n",
    "    bids_path = BIDSPath(\n",
    "        run = block,\n",
    "        subject = sub, \n",
    "        task = task, \n",
    "        datatype = 'eeg', \n",
    "        root = BIDS_DIR\n",
    "    )\n",
    "    \n",
    "    # get range of dates the BIDS specfiication will accept\n",
    "    daysback_min, daysback_max = get_anonymization_daysback(raw)\n",
    "    \n",
    "    # write data into BIDS directory, while anonymizing\n",
    "    write_raw_bids(\n",
    "        raw, \n",
    "        bids_path = bids_path, \n",
    "#         allow_preload = True, # whether to load full dataset into memory when copying\n",
    "#         format = 'BrainVision', # format to save to\n",
    "        anonymize = dict(daysback = daysback_min) # shift dates by daysback\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ced54",
   "metadata": {},
   "source": [
    "A couple of notes:\n",
    "\n",
    "1. In this case, we are just copying from one Brain Vision file to another, which we can do since our data we already in that file format. Often, we'll collect from a system that outputs to a file format which isn't already BIDS compliant. In that case, you'll need to load the data into memory with `raw.load_data()` and then set the `allow_preload = True` when writing the data. Last I checked, this is also necessary when renaming channels for idiosyncratic reasons (though this may change since MNE-BIDS is under active development), which is why we've done so here even though our data is already in a Brain Vision file.\n",
    "2. If you have digitized electrode positions for your specific subject, you'll want to [load those as you normally would in MNE](https://mne.tools/stable/auto_tutorials/intro/40_sensor_locations.html) and assign them to the `raw` object before writing to BIDS. This will ensure your electrode locations get recorded in a BIDS compliant manner. This is _only_ for subject-specific electrode positions; don't do this for standard templates.\n",
    "3. As we saw in the timing test tutorial, MNE loads event times stored in the Brain Vision file as annotations in `raw.annotations`, which MNE-BIDS records in a BIDS-valid event file automatically. If your events are represented in a different way, you can either convert them to annoations or provide an events data structure to `write_raw_bids`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db96b8b",
   "metadata": {},
   "source": [
    "That's pretty much it. We can check view some attributes of our resulting data directory using the `pybids` package. (Accessing the directory as a `BIDSLayout` also runs the [BIDS Validator](https://github.com/bids-standard/bids-validator) automatically, ensuring everything is up to par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95fb9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids import BIDSLayout\n",
    "layout = BIDSLayout(BIDS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd46182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '1', '3', '4']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62dbe166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'pitches', 'pitch']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout.get_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_bids import print_dir_tree\n",
    "print_dir_tree(BIDS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8b3af",
   "metadata": {},
   "source": [
    "You can go through the descriptor files like `README`, `dataset_description.json`, and `participants.tsv` to add other information (e.g. the paper's authors, subjects' handedness, etc.) by hand if you wish. MNE-BIDS will also happily organize data from different sessions and, runs, and tasks into one, big, happy directory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
