{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e675a4",
   "metadata": {},
   "source": [
    "Okay, so we've collected some data. Woohoo. The first thing we'll want to do is take the files our acquisition software has just output and organize them in a sensible way. \n",
    "\n",
    "What is \"a sensible way,\" you mught ask? Any organization where you (and anybody else who ever needs to use your data) will immediately know where everything is and be able to find all the important metadata (e.g. acquisition parameters, which may ont be available in the file itself, etc.).\n",
    "\n",
    "Labs will often have their own internal guidelines for how an EEG dataset should be organized, or maybe people just do what works for them. I'm a big fan of the [Brain Imaging Data Structure (BIDS)](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/03-electroencephalography.html); if a committee of experienced researchers put their heads together and decided this was a sensible way to organize data, who am I to reinvent the wheel? And by organizing our data the same way as everyone else, we are afforded the ability to use tools that assume your data follows BIDS specifications. Importantly, when we write code to analyze our dataset, we know we can apply it to any other dataset stored in BIDS format in the future, like every dataset on [OpenNeuro](https://openneuro.org/) which can save you a ton of time down the line.\n",
    "\n",
    "The only annoying part about BIDS is that it can be a struggle to get your data into the highly specific directory structure. Luckily, the EEG ecosystem has tools like [MNE-BIDS](https://mne.tools/mne-bids/stable/index.html) in Python and [Fieldtrip Toolbox's `data2bids` function](https://www.fieldtriptoolbox.org/example/bids/) to make this part trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec100e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_bids import BIDSPath, write_raw_bids, get_anonymization_daysback\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb5460",
   "metadata": {},
   "source": [
    "First we'll load our data into MNE as we would any other EEG file. I've put the data we collected from Pablo into a folder called `data`, which we'll peak inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94ccd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'letty_subj_1_block_1.eeg',\n",
       " 'letty_subj_1_block_1.vhdr',\n",
       " 'letty_subj_1_block_1.vmrk']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '../raw_data' # where our data currently lives\n",
    "BIDS_DIR = '../bids_data' # where we want it to live\n",
    "\n",
    "fnames = os.listdir(DATA_DIR)\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fe92c",
   "metadata": {},
   "source": [
    "We only have one subject now, so it would be easy to hardcode this. But we can save ourselves some work down the line by automating this process, so we'll pretend we have more subjects than we do. \n",
    "\n",
    "MNE only needs one of the file names to read the file; namely, the `.vhdr` header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331fcb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letty_subj_1_block_1.vhdr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = [f for f in fnames if '.vhdr' in f] # filter for .vhdr files\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd29443",
   "metadata": {},
   "source": [
    "How our subject IDs and task names are represented in our filename will obviously vary from project to project, since they depend on what you type into the acquistion software. (This type of inconsistent naming conventions is why we're converting to BIDS to begin with.) So you'll need to write your own code for this next part. \n",
    "\n",
    "I'm using regular expressions because I normally find them handy for pulling info out of file names, but obviously there are other (easier, if you don't already know the notoriously inscrutable regular expression syntax) ways to do this. Don't mind me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a6fed2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs = re.findall('\\w+_\\w+_(\\w+).vhdr', fnames[0])\n",
    "tasks = ['pitches']\n",
    "subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92faeae9",
   "metadata": {},
   "source": [
    "We'll want to rename our channels to something more information than `'Ch1'`, etc. Brain Products' acticaps positions electrodes according to the [10-20 system](https://en.wikipedia.org/wiki/10%E2%80%9320_system_(EEG)), so if we rename our electrodes to their 10-20 location names, everyone will know where they are on the head. We'll make a mapping from the channel names in our files to the corresponding 10-20 names using the layout file provided by Brain Products for our cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390f0b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ch0': 'GND',\n",
       " 'Ch1': 'Fp1',\n",
       " 'Ch2': 'Fz',\n",
       " 'Ch3': 'F3',\n",
       " 'Ch4': 'F7',\n",
       " 'Ch5': 'FT9',\n",
       " 'Ch6': 'FC5',\n",
       " 'Ch7': 'FC1',\n",
       " 'Ch8': 'C3',\n",
       " 'Ch9': 'T7',\n",
       " 'Ch10': 'TP9',\n",
       " 'Ch11': 'CP5',\n",
       " 'Ch12': 'CP1',\n",
       " 'Ch13': 'Pz',\n",
       " 'Ch14': 'P3',\n",
       " 'Ch15': 'P7',\n",
       " 'Ch16': 'O1',\n",
       " 'Ch17': 'Oz',\n",
       " 'Ch18': 'O2',\n",
       " 'Ch19': 'P4',\n",
       " 'Ch20': 'P8',\n",
       " 'Ch21': 'TP10',\n",
       " 'Ch22': 'CP6',\n",
       " 'Ch23': 'CP2',\n",
       " 'Ch24': 'Cz',\n",
       " 'Ch25': 'C4',\n",
       " 'Ch26': 'T8',\n",
       " 'Ch27': 'FT10',\n",
       " 'Ch28': 'FC6',\n",
       " 'Ch29': 'FC2',\n",
       " 'Ch30': 'F4',\n",
       " 'Ch31': 'F8',\n",
       " 'Ch32': 'Fp2',\n",
       " 'Ch33': 'AF7',\n",
       " 'Ch34': 'AF3',\n",
       " 'Ch35': 'AFz',\n",
       " 'Ch36': 'F1',\n",
       " 'Ch37': 'F5',\n",
       " 'Ch38': 'FT7',\n",
       " 'Ch39': 'FC3',\n",
       " 'Ch40': 'C1',\n",
       " 'Ch41': 'C5',\n",
       " 'Ch42': 'TP7',\n",
       " 'Ch43': 'CP3',\n",
       " 'Ch44': 'P1',\n",
       " 'Ch45': 'P5',\n",
       " 'Ch46': 'PO7',\n",
       " 'Ch47': 'PO3',\n",
       " 'Ch48': 'POz',\n",
       " 'Ch49': 'PO4',\n",
       " 'Ch50': 'PO8',\n",
       " 'Ch51': 'P6',\n",
       " 'Ch52': 'P2',\n",
       " 'Ch53': 'CPz',\n",
       " 'Ch54': 'CP4',\n",
       " 'Ch55': 'TP8',\n",
       " 'Ch56': 'C6',\n",
       " 'Ch57': 'C2',\n",
       " 'Ch58': 'FC4',\n",
       " 'Ch59': 'FT8',\n",
       " 'Ch60': 'F6',\n",
       " 'Ch61': 'AF8',\n",
       " 'Ch62': 'AF4',\n",
       " 'Ch63': 'F2',\n",
       " 'Ch64': 'FCz'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig = mne.channels.read_custom_montage(DATA_DIR + '/CACS-64_NO_REF.bvef')\n",
    "mapping = {'Ch%s'%i: dig.ch_names[i] for i in range(len(dig.ch_names))}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ebafd",
   "metadata": {},
   "source": [
    "The ground electrode doesn't appear in the file, so we will remove that from the mapping (because MNE isn't yet smart enough to deal with extraneous values). Also, I had electrode 24 set as the reference electrode during the recording, so it didn't appear in the file. (As a side note, while we don't do it here, we can actually add the reference channel back with a constant value of zero, since it will become a valid channel again after re-referencing.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f8638f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mapping['Ch0']\n",
    "del mapping['Ch24']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0afa6",
   "metadata": {},
   "source": [
    "Now all we need to do is read the data file using MNE, add any info that BIDS will want but isn't available in the original file (in this case, the power line frequency, which varies from country to country so constitutes important and non-obvious metadata), give the basic info we just extracted to MNE-BIDS so it can build the BIDS directory structure, and copy our data. We'll also want to rename our events to something more interpretable than integer codes.\n",
    "\n",
    "If we want to share this dataset in the future, we'll also need to anonymize it. That means removing the date it is collected. (It would also mean removing our subject's name, but the cat's already out of the bag on that one in this case -- sorry, Pablo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d323f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from ../raw_data/letty_subj_1_block_1.vhdr...\n",
      "Setting channel info structure...\n",
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S  1', 'Stimulus/S  2', 'Stimulus/S  3', 'Stimulus/S  4', 'Stimulus/S  5', 'Stimulus/S  6', 'Stimulus/S  7', 'Stimulus/S  8', 'Stimulus/S  9', 'Stimulus/S 10', 'Stimulus/S 11']\n",
      "Extracting parameters from /Users/letitiaho/src/pitch-tracking/raw_data/letty_subj_1_block_1.vhdr...\n",
      "Setting channel info structure...\n",
      "Writing '../bids_data/participants.tsv'...\n",
      "Writing '../bids_data/participants.json'...\n",
      "Used Annotations descriptions: ['baseline', 'oddball']\n",
      "Writing '../bids_data/sub-1/eeg/sub-1_task-pitches_events.tsv'...\n",
      "Writing '../bids_data/dataset_description.json'...\n",
      "Writing '../bids_data/sub-1/eeg/sub-1_task-pitches_eeg.json'...\n",
      "Writing '../bids_data/sub-1/eeg/sub-1_task-pitches_channels.tsv'...\n",
      "Copying data files to sub-1_task-pitches_eeg.vhdr\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '/Users/letitiaho/src/pitch-tracking/raw_data/letty_subj_1_block_1.eeg' -> '../bids_data/sub-1/eeg/sub-1_task-pitches_eeg.eeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-aef5b759a29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# write data into BIDS directory, while anonymizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     write_raw_bids(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mbids_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbids_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-592>\u001b[0m in \u001b[0;36mwrite_raw_bids\u001b[0;34m(raw, bids_path, events_data, event_id, anonymize, format, symlink, empty_room, allow_preload, montage, acpc_aligned, overwrite, verbose)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mne_bids/write.py\u001b[0m in \u001b[0;36mwrite_raw_bids\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;31m# BrainVision is multifile, copy over all of them and fix pointers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.vhdr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mcopyfile_brainvision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manonymize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manonymize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'.edf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.EDF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.bdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.BDF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manonymize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-588>\u001b[0m in \u001b[0;36mcopyfile_brainvision\u001b[0;34m(vhdr_src, vhdr_dest, anonymize, verbose)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/mne_bids/copyfiles.py\u001b[0m in \u001b[0;36mcopyfile_brainvision\u001b[0;34m(vhdr_src, vhdr_dest, anonymize, verbose)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Copy data .eeg ... no links to repair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0msh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_dest\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.eeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# Write new header and marker files, fixing the file pointer links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0m_fastcopy_fcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COPYFILE_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_fcopyfile\u001b[0;34m(fsrc, fdst, flags)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_fcopyfile\u001b[0;34m(fsrc, fdst, flags)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mposix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/Users/letitiaho/src/pitch-tracking/raw_data/letty_subj_1_block_1.eeg' -> '../bids_data/sub-1/eeg/sub-1_task-pitches_eeg.eeg'"
     ]
    }
   ],
   "source": [
    "for i in range(len(fnames)):\n",
    "    \n",
    "    sub = subs[i]\n",
    "    task = tasks[i]\n",
    "    fpath = os.path.join(DATA_DIR, fnames[i])\n",
    "    \n",
    "    # load data with MNE function for your file format\n",
    "    raw = mne.io.read_raw_brainvision(fpath)\n",
    "\n",
    "    # add some info BIDS will want\n",
    "    raw.info['line_freq'] = 60 # the power line frequency in the building we collected in\n",
    "    \n",
    "    # rename events from random integers to interpretable names\n",
    "    # (this part is specific to your experiment, obviously)\n",
    "    events, event_ids = mne.events_from_annotations(raw)\n",
    "    events = events[events[:,2] != event_ids['New Segment/'], :]\n",
    "    event_codes = events[:,2]\n",
    "    baseline_code = np.argmax(np.bincount(event_codes)) # the one with more trials\n",
    "    oddball_code = np.unique(event_codes)[np.unique(event_codes) != baseline_code][0]\n",
    "    event_names = {baseline_code: 'baseline', oddball_code: 'oddball'}\n",
    "    annot = mne.annotations_from_events(events, sfreq = raw.info['sfreq'], event_desc = event_names)\n",
    "    raw = raw.set_annotations(annot)\n",
    "#     raw.load_data() # read data from memory\n",
    "    raw.rename_channels(mapping)\n",
    "    \n",
    "    # build appropriate BIDS directory structure \n",
    "    bids_path = BIDSPath(\n",
    "        subject = sub, \n",
    "        task = task, \n",
    "        datatype = 'eeg', \n",
    "        root = BIDS_DIR\n",
    "    )\n",
    "    \n",
    "    # get range of dates the BIDS specfiication will accept\n",
    "    daysback_min, daysback_max = get_anonymization_daysback(raw)\n",
    "    \n",
    "    # write data into BIDS directory, while anonymizing\n",
    "    write_raw_bids(\n",
    "        raw, \n",
    "        bids_path = bids_path, \n",
    "#         allow_preload = True, # whether to load full dataset into memory when copying\n",
    "#         format = 'BrainVision', # format to save to\n",
    "        anonymize = dict(daysback = daysback_min) # shift dates by daysback\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ced54",
   "metadata": {},
   "source": [
    "A couple of notes:\n",
    "\n",
    "1. In this case, we are just copying from one Brain Vision file to another, which we can do since our data we already in that file format. Often, we'll collect from a system that outputs to a file format which isn't already BIDS compliant. In that case, you'll need to load the data into memory with `raw.load_data()` and then set the `allow_preload = True` when writing the data. Last I checked, this is also necessary when renaming channels for idiosyncratic reasons (though this may change since MNE-BIDS is under active development), which is why we've done so here even though our data is already in a Brain Vision file.\n",
    "2. If you have digitized electrode positions for your specific subject, you'll want to [load those as you normally would in MNE](https://mne.tools/stable/auto_tutorials/intro/40_sensor_locations.html) and assign them to the `raw` object before writing to BIDS. This will ensure your electrode locations get recorded in a BIDS compliant manner. This is _only_ for subject-specific electrode positions; don't do this for standard templates.\n",
    "3. As we saw in the timing test tutorial, MNE loads event times stored in the Brain Vision file as annotations in `raw.annotations`, which MNE-BIDS records in a BIDS-valid event file automatically. If your events are represented in a different way, you can either convert them to annoations or provide an events data structure to `write_raw_bids`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db96b8b",
   "metadata": {},
   "source": [
    "That's pretty much it. We can check view some attributes of our resulting data directory using the `pybids` package. (Accessing the directory as a `BIDSLayout` also runs the [BIDS Validator](https://github.com/bids-standard/bids-validator) automatically, ensuring everything is up to par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fb9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids import BIDSLayout\n",
    "layout = BIDSLayout(BIDS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd46182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62dbe166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pitches']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout.get_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86f99fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|bids_data/\n",
      "|--- README\n",
      "|--- dataset_description.json\n",
      "|--- participants.json\n",
      "|--- participants.tsv\n",
      "|--- sub-1/\n",
      "|------ sub-1_scans.tsv\n",
      "|------ eeg/\n",
      "|--------- sub-1_task-pitches_channels.tsv\n",
      "|--------- sub-1_task-pitches_eeg.eeg\n",
      "|--------- sub-1_task-pitches_eeg.json\n",
      "|--------- sub-1_task-pitches_eeg.vhdr\n",
      "|--------- sub-1_task-pitches_eeg.vmrk\n",
      "|--------- sub-1_task-pitches_events.tsv\n"
     ]
    }
   ],
   "source": [
    "from mne_bids import print_dir_tree\n",
    "print_dir_tree(BIDS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8b3af",
   "metadata": {},
   "source": [
    "You can go through the descriptor files like `README`, `dataset_description.json`, and `participants.tsv` to add other information (e.g. the paper's authors, subjects' handedness, etc.) by hand if you wish. MNE-BIDS will also happily organize data from different sessions and, runs, and tasks into one, big, happy directory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
